{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import ngram_lm, ngram_lm_test\n",
    "import ngram_utils\n",
    "from shared_lib import utils, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-k Smoothing\n",
    "\n",
    "Below is the unsmoothed maximum likelihood estimate of $ P(w_i\\ |\\ w_{i-1}, w_{i-2})$ where it uses the raw distribution over words seen in a context in the training data:\n",
    "\n",
    "$$  \\hat{P}(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
    "\n",
    "Add-k smoothing is the simple refinement where it add $k > 0$ to each count $C_{abc}$, pretending it has seen every vocabulary word $k$ extra times in each context. So we have:\n",
    "\n",
    "$$ \\hat{P}_k(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc} + k}{\\sum_{c'} (C_{abc'} + k)} = \\frac{C_{abc} + k}{C_{ab} + k\\cdot|V|} $$\n",
    "\n",
    "where $|V|$ is the size of our vocabulary.\n",
    "\n",
    "In the code below, we'll refer to $(w_{i-2}, w_{i-1})$ as the *context*, and $w_i$ as the current *word*. By convention, we'll somewhat interchangeably refer to the sequence $(w_{i-2}, w_{i-1}, w)$ as $abc$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Implementing the Add-k Model\n",
    "\n",
    "Despite its shortcomings, it's worth implementing an add-k model as a baseline. Unlike the unsmoothed model, we'll be able to get some reasonable (or at least, finite) perplexity numbers which we can compare to the Kneser-Ney model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_totals (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_counts (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_next_word_proba_k_exists (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_next_word_proba_no_smoothing (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_no_mutate_on_predict (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_words (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "reload(ngram_lm_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestAddKTrigramLM', ngram_lm_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser-Ney Smoothing\n",
    "\n",
    "Explore Kneser-Ney smoothing as a more sophisticated way of estimating unseen probabilities. \n",
    "\n",
    "When building an n-gram model, we're limited by the model order (e.g. trigram, 4-gram, or 5-gram) and how much data is available. Within that, we want to use as much information as possible. Within, say, a trigram context, we can compute a number of different statistics that might be helpful. Let's review a few goals:\n",
    "1. If we don't have good n-gram estimates, we want to back off to (n-1) grams.\n",
    "2. If we back off to (n-1) grams, we should do it \"smoothly\".\n",
    "3. Our counts $C_{abc}$ are probably _overestimates_ for the n-grams we observe (see *held-out reweighting*).\n",
    "4. Type fertilities tell us more about $P(w_{new}\\ |\\ \\text{context})$ than the unigram distribution does.\n",
    "\n",
    "Kneser-Ney smoothing combines all four of these ideas. \n",
    "\n",
    "**Absolute discounting** - which follows from 3. - gives us an easy way to backoff (1. and 2.), by distributing the subtracted probability mass among the backoff distribution $\\tilde{P}(c\\ |\\ b)$.  The amount to redistribute, $\\delta$, is a hyperparameter selected based on a cross-validation set in the usual way, although here we'll just let $\\delta = 0.75$.\n",
    "\n",
    "$$ P_{ad}(c\\ |\\ b, a) = \\frac{C_{abc} - \\delta}{C_{ab}} + \\alpha_{ab} \\tilde{P}(c\\ |\\  b) $$\n",
    "\n",
    "Where $\\alpha_{ab}$ is a backoff factor, derived from the counts, that guarantees that the probabilities are normalized: $\\sum_{c'} P_{ad}(c'\\ |\\ b, a) = 1$. This definition is recursive: if we let $\\tilde{P}(c\\ |\\  b) = P_{ad}(c\\ |\\ b)$, then the backoff distribution can also back off to even lower n-grams.\n",
    "\n",
    "*Note:* we need the numerator above to positive, so it should actually read $\\max(0, C_{abc} - \\delta)$.\n",
    "\n",
    "**Type fertility** is item 4. Instead of falling back to the unigram distribution at the end, we'll define $\\hat{P}(w)$ as proportional to the type fertility of $w$, or the *number of unique preceding words* $w_{i-1}$.  In the following equation, the word we are estimating the probability of is $c$.  $b'$ are the set of words we've found occurring before $c$ in the training data.\n",
    "\n",
    "$$ \\hat{P}_{tf}(c) \\propto \\left|\\ b' : C_{b'c} > 0\\ \\right| = tf(c)$$\n",
    "\n",
    "In order to make this a valid probability distribution, we need to normalize it with a factor $Z_{tf} = \\sum_{w} tf(w)$, so we have $\\hat{P}_{tf}(w) = \\frac{tf(w)}{Z_{tf}} $\n",
    "\n",
    "### KN Equations\n",
    "\n",
    "Putting it all together, we have our equations for a KN trigram model:\n",
    "\n",
    "$$ P_{kn}(c\\ |\\ b, a) = \\frac{\\max(0, C_{abc} - \\delta)}{C_{ab}} + \\alpha_{ab} P_{kn}(c\\ |\\  b) $$\n",
    "where the bigram backoff is:\n",
    "$$ P_{kn}(c\\ |\\ b) = \\frac{\\max(0, C_{bc} - \\delta)}{C_{b}} + \\alpha_{b} P_{kn}(c) $$\n",
    "and the unigram (type fertility) backoff is:\n",
    "$$ P_{kn}(c) = \\frac{tf(c)}{Z_{tf}} \\quad \\text{where} \\quad tf(c) = \\left|\\ b' : C_{b'c} > 0\\ \\right| $$\n",
    "\n",
    "Note that there is only one free parameter in this model, $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Implementing the KN Model\n",
    "\n",
    "Implement the `KNTrigramLM` in `ngram_lm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_nnz (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_context_totals (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_counts (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_kn_interp (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_next_word_proba (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_no_mutate_on_predict (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_type_contexts (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_type_fertility (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_words (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_z_tf (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.024s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=10 errors=0 failures=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "reload(ngram_lm_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestKNTrigramLM', ngram_lm_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "The same code below can be used with either model; in the cell where it says \"Select your Model\", you can choose the add-k model or the KN model.\n",
    "\n",
    "## Loading & Preprocessing\n",
    "Once again, we'll build our model on the Brown corpus. We'll do an 80/20 train/test split, and preprocess words by lowercasing and replacing digits with `DG` (so `2016` becomes `DGDGDGDG`).\n",
    "\n",
    "In a slight departure from the `lm1.ipynb` demo, we'll restrict the vocabulary to 40000 words. This way, a small fraction of the *training* data will be mapped to `<unk>` tokens, and the model can learn n-gram probabilities that include `<unk>` for prediction on the test set. (If we interpret `<unk>` as meaning \"rare word\", then this is somewhat plausible as a way to infer things about the class of rare words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/guangzhi_xie/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (979646 tokens)\n",
      "Test set: 11468 sentences (181546 tokens)\n",
      "Train set vocabulary: 30000 words\n"
     ]
    }
   ],
   "source": [
    "assert(nltk.download('brown'))  # Make sure we have the data.\n",
    "corpus = nltk.corpus.brown\n",
    "V = 30000\n",
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
    "# vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(corpus.sents())), size=V)\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our smoothed models will also be trigram models, so for convenience we'll also prepend *two* `<s>` markers. (We could avoid this, but then we'd need special handling for the first token of each sentence.)\n",
    "\n",
    "To make it easier to work with, we'll take the list of tokens as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: \n",
      "array(['<s>', '<s>', u'the', u'fulton', u'county', u'grand', u'jury',\n",
      "       u'said', u'friday', u'an', u'investigation', u'of', u\"atlanta's\",\n",
      "       u'recent', u'primary', u'election', u'produced', u'``', u'no',\n",
      "       u'evidence'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "print \"Sample data: \\n\" + repr(train_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the model\n",
    "\n",
    "Select either `AddKTrigramLM` or `KNTrigramLM` in the cell below. If switching models, you only need to re-run the cells below here - no need to re-run the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 6.83 s\n",
      "=== N-gram Language Model stats ===\n",
      "30000 unique 1-grams\n",
      "358274 unique 2-grams\n",
      "733388 unique 3-grams\n",
      "Optimal memory usage (counts only): 24 MB\n"
     ]
    }
   ],
   "source": [
    "import ngram_lm\n",
    "reload(ngram_lm)\n",
    "\n",
    "# Uncomment the line below for the model you want to run.\n",
    "# Model = ngram_lm.AddKTrigramLM\n",
    "Model = ngram_lm.KNTrigramLM\n",
    "\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `params` to change the smoothing factor. `AddKTrigramLM` will ignore the value of `delta`, and `KNTrigramLM` will ignore `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.set_live_params(k = 0.001, delta=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> but fatty foods unless it gives him , ( DG ) the possibility of the corresponding changes in world development\n",
      "[20 tokens; log P(seq): -109.24]\n",
      "\n",
      "<s> <s> <s> when he died young bearden experienced idea-exchange violin called up to the porch of the american '' . </s>\n",
      "[18 tokens; log P(seq): -144.88]\n",
      "\n",
      "<s> <s> a christian youth for christ lives . </s>\n",
      "[7 tokens; log P(seq): -46.83]\n",
      "\n",
      "<s> <s> alec leaned on the jelke cleared circus if he angry , try to play , was treated effluent varied to\n",
      "[20 tokens; log P(seq): -144.93]\n",
      "\n",
      "<s> <s> mr. lay with its very existence to blind decomposition of this distinguished associated graduate stake in the principal post to\n",
      "[20 tokens; log P(seq): -143.36]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring on Held-Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 17.17\n",
      "Test perplexity: 286.60\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))\n",
    "\n",
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Curiosities\n",
    "\n",
    "You might have seen this floating around the internet:\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "Let's see if it holds true, statistically at least. Note that log probabilities are always negative, so the smaller magnitude is better. And remember the log scale: a difference of score of 8 units means one utterance is $2^8 = 256$ times more likely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_scoring(sentence):\n",
    "    # Pre-process words, replace anything the model doesn't know\n",
    "    # with <unk>\n",
    "    words = [utils.canonicalize_word(w, wordset=known_words)\n",
    "             for w in sentence]\n",
    "    # Pad sequence with start and end markers\n",
    "    return [\"<s>\", \"<s>\"] + words + [\"</s>\"]\n",
    "\n",
    "known_words = vocab.wordset\n",
    "s0 = preprocess_for_scoring(\"square green plastic toys\".split())\n",
    "s1 = preprocess_for_scoring(\"plastic green square toys\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s0 score: -51.91\n",
      "s1 score: -60.99\n"
     ]
    }
   ],
   "source": [
    "print \"s0 score: %.02f\" % ngram_utils.score_seq(lm, s0)[0]\n",
    "print \"s1 score: %.02f\" % ngram_utils.score_seq(lm, s1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"square green plastic toys\" : -51.91\n",
      "\"green square plastic toys\" : -51.94\n",
      "\"plastic square green toys\" : -60.99\n",
      "\"plastic green square toys\" : -60.99\n",
      "\"square plastic green toys\" : -61.21\n",
      "\"green plastic square toys\" : -61.24\n"
     ]
    }
   ],
   "source": [
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "results = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = list(adjs) + [noun]\n",
    "    seq = preprocess_for_scoring(words)\n",
    "    score = ngram_utils.score_seq(lm, seq)\n",
    "    results.append((score[0], words))\n",
    "\n",
    "# Sort results\n",
    "for score, words in sorted(results, reverse=True):\n",
    "    print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
